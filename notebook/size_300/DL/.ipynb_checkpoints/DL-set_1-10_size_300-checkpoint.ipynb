{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confidential-brief",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sized-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root project directory to path\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from diffnet.dataset_loader import *\n",
    "from diffnet.diffnet import *\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-importance",
   "metadata": {},
   "source": [
    "## Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unable-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 26 15:48:13 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  On   | 00000000:09:00.0 Off |                  N/A |\r\n",
      "| 35%   38C    P8    21W / 260W |     15MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  On   | 00000000:0A:00.0 Off |                  N/A |\r\n",
      "| 35%   36C    P8    21W / 260W |      5MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1440      G   /usr/lib/xorg/Xorg                  9MiB |\r\n",
      "|    0   N/A  N/A      1597      G   /usr/bin/gnome-shell                4MiB |\r\n",
      "|    1   N/A  N/A      1440      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "contained-rhythm",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"1\"\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "# allocate as small memory as possible\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "centered-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix seed\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-antibody",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hundred-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary functions for machine learning model\n",
    "\n",
    "def cal_mse(y_true, y_pred):\n",
    "    loss = tf.reduce_mean(tf.square(y_true-y_pred))\n",
    "    return loss\n",
    "\n",
    "def train_step(model, x, y, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = cal_mse(y_true=y, y_pred=y_pred)\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wicked-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self.patience  = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def validate(self, loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print(f'Training process is stopped early....')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "animated-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, is_transfer, source_weight_path, \n",
    "             n_steps, step, min_step, min_loss, train_iter, val_iter, \n",
    "             prefix, save_dir, es, random_state):\n",
    "    \n",
    "    seed_everything(random_state)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_step_idx = []\n",
    "    val_loss_list = []\n",
    "    val_step_idx = []\n",
    "    \n",
    "    if is_transfer:\n",
    "        model.load_weights(source_weight_path)\n",
    "\n",
    "    start = time.time()\n",
    "    while step < n_steps+1:\n",
    "        if step == 0:\n",
    "            step += 1\n",
    "            continue\n",
    "\n",
    "        x, y = next(train_iter)\n",
    "        y_pred, loss = train_step(model, x, y, optimizer)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_step_idx.append(step)\n",
    "        #print(loss)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            val_x, val_y = next(val_iter)\n",
    "            val_y_pred = diffnet(val_x, training=False)\n",
    "\n",
    "            val_loss = cal_mse(y_true=val_y, y_pred=val_y_pred)\n",
    "\n",
    "            print(\"[{:d}] Loss Val: {:7.4f}, MIN MSE ({:d}): {:.4f}\".format(step, val_loss, min_step, min_loss))\n",
    "\n",
    "            val_loss_list.append(val_loss)\n",
    "            val_step_idx.append(step)\n",
    "\n",
    "            if val_loss < min_loss:\n",
    "                min_step = step\n",
    "                print(\"NEW MIN LOSS : %7.4f\" % val_loss)\n",
    "                model.save_weights(\"{}/diffnet-{}-min.h5\".format(prefix+save_dir, save_dir))\n",
    "                min_loss = val_loss\n",
    "\n",
    "                now = time.time()\n",
    "                with open(\"{}/training_time.txt\".format(prefix+save_dir), \"a\") as h:\n",
    "                    h.write(\"{step}\\t{time}\\t{loss}\\n\".format(step=min_step, time=now-start, loss=min_loss))\n",
    "\n",
    "                h.close()\n",
    "                \n",
    "            if es.validate(val_loss):\n",
    "                break\n",
    "\n",
    "        step+=1\n",
    "        \n",
    "    return train_step_idx, train_loss_list, val_step_idx, val_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-unknown",
   "metadata": {},
   "source": [
    "## Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "injured-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine size of training data & determine transfer or not\n",
    "\n",
    "n_data = 300\n",
    "is_transfer = False\n",
    "source_bar = 1\n",
    "start_idx = 1\n",
    "end_idx = 10\n",
    "\n",
    "if is_transfer:\n",
    "    prefix = \"../../../checkpoint/Transfer_learning/size_{n_data}/source_{sb}bar/\".format(n_data=n_data, sb=source_bar)\n",
    "else:\n",
    "    prefix = \"../../../checkpoint/Direct_learning/size_{}/\".format(n_data)\n",
    "    \n",
    "save_dir_list = [f\"set_{i}\" for i in range(start_idx, end_idx+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "corrected-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_INPUT_FILES = [\"../../../data/features/core_mof_geo+gridhist_50.txt\"]\n",
    "LABEL_INPUT_FILES = [\"../../../data/labels/coremof_d_ch4_rawset.txt\"]\n",
    "\n",
    "f_input_df = pd.read_csv(FEATURE_INPUT_FILES[0], sep=\"\\s+\", index_col=0)\n",
    "l_input_df = pd.read_csv(LABEL_INPUT_FILES[0], sep=\"\\s+\", index_col=0, header=None)\n",
    "\n",
    "for i in range(len(FEATURE_INPUT_FILES)-1):\n",
    "    idx = i+1\n",
    "    \n",
    "    try:\n",
    "        f_df = pd.read_csv(FEATURE_INPUT_FILES[idx], sep=\"\\s+\", index_col=0)\n",
    "        l_df = pd.read_csv(LABEL_INPUT_FILES[idx], sep=\"\\s+\", index_col=0, header=None)\n",
    "\n",
    "        f_input_df = pd.concat([f_input_df, f_df])\n",
    "        l_input_df = pd.concat([l_input_df, l_df])\n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "f_input_df = f_input_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exclusive-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize geometric features\n",
    "geo_cols = f_input_df.columns[:5]\n",
    "geo_df = f_input_df[:][geo_cols]\n",
    "\n",
    "max_v = [60, 60, 4000, 10000, 1]\n",
    "    \n",
    "f_input_df[:][geo_cols] = f_input_df[:][geo_cols] / max_v\n",
    "\n",
    "# only for geo only\n",
    "#f_input_df = f_input_df[:][geo_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beneficial-stevens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LCD</th>\n",
       "      <th>PLD</th>\n",
       "      <th>Vol_ASA</th>\n",
       "      <th>Grav_ASA</th>\n",
       "      <th>Void_fraction</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.02</th>\n",
       "      <th>0.04</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.08</th>\n",
       "      <th>...</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.82</th>\n",
       "      <th>0.84</th>\n",
       "      <th>0.86</th>\n",
       "      <th>0.88</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.92</th>\n",
       "      <th>0.94</th>\n",
       "      <th>0.96</th>\n",
       "      <th>0.98</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOF</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BEFMAC_clean</th>\n",
       "      <td>0.089951</td>\n",
       "      <td>0.057887</td>\n",
       "      <td>0.314925</td>\n",
       "      <td>0.083036</td>\n",
       "      <td>0.06358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.012245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IKETUO_clean</th>\n",
       "      <td>0.090777</td>\n",
       "      <td>0.070132</td>\n",
       "      <td>0.432138</td>\n",
       "      <td>0.102958</td>\n",
       "      <td>0.11112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.826211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ja073612tsi20070723_060520_clean</th>\n",
       "      <td>0.097501</td>\n",
       "      <td>0.050559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.11166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.828755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOMDOU_clean</th>\n",
       "      <td>0.101477</td>\n",
       "      <td>0.077729</td>\n",
       "      <td>0.354425</td>\n",
       "      <td>0.119939</td>\n",
       "      <td>0.08200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.893230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUVHEH_clean</th>\n",
       "      <td>0.212980</td>\n",
       "      <td>0.200457</td>\n",
       "      <td>0.391778</td>\n",
       "      <td>0.094706</td>\n",
       "      <td>0.26824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.682870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       LCD       PLD   Vol_ASA  Grav_ASA  \\\n",
       "MOF                                                                        \n",
       "BEFMAC_clean                      0.089951  0.057887  0.314925  0.083036   \n",
       "IKETUO_clean                      0.090777  0.070132  0.432138  0.102958   \n",
       "ja073612tsi20070723_060520_clean  0.097501  0.050559  0.000000  0.000000   \n",
       "XOMDOU_clean                      0.101477  0.077729  0.354425  0.119939   \n",
       "CUVHEH_clean                      0.212980  0.200457  0.391778  0.094706   \n",
       "\n",
       "                                  Void_fraction  0.0  0.02      0.04  \\\n",
       "MOF                                                                    \n",
       "BEFMAC_clean                            0.06358  0.0   0.0  0.004082   \n",
       "IKETUO_clean                            0.11112  0.0   0.0  0.000000   \n",
       "ja073612tsi20070723_060520_clean        0.11166  0.0   0.0  0.000000   \n",
       "XOMDOU_clean                            0.08200  0.0   0.0  0.000000   \n",
       "CUVHEH_clean                            0.26824  0.0   0.0  0.000000   \n",
       "\n",
       "                                      0.06      0.08  ...       0.8      0.82  \\\n",
       "MOF                                                   ...                       \n",
       "BEFMAC_clean                      0.012245  0.000000  ...  0.000000  0.000000   \n",
       "IKETUO_clean                      0.000000  0.000000  ...  0.000000  0.002849   \n",
       "ja073612tsi20070723_060520_clean  0.000000  0.000000  ...  0.000916  0.003663   \n",
       "XOMDOU_clean                      0.001137  0.003204  ...  0.000413  0.000413   \n",
       "CUVHEH_clean                      0.000000  0.000000  ...  0.002104  0.002946   \n",
       "\n",
       "                                      0.84      0.86      0.88       0.9  \\\n",
       "MOF                                                                        \n",
       "BEFMAC_clean                      0.000000  0.000000  0.008163  0.000000   \n",
       "IKETUO_clean                      0.005698  0.000000  0.000000  0.000000   \n",
       "ja073612tsi20070723_060520_clean  0.002747  0.000916  0.000000  0.003663   \n",
       "XOMDOU_clean                      0.000827  0.000827  0.001240  0.000620   \n",
       "CUVHEH_clean                      0.001543  0.002245  0.002946  0.002525   \n",
       "\n",
       "                                      0.92      0.94      0.96      0.98  \n",
       "MOF                                                                       \n",
       "BEFMAC_clean                      0.000000  0.000000  0.000000  0.879592  \n",
       "IKETUO_clean                      0.000000  0.002849  0.000000  0.826211  \n",
       "ja073612tsi20070723_060520_clean  0.000916  0.003663  0.003663  0.828755  \n",
       "XOMDOU_clean                      0.000620  0.001860  0.000413  0.893230  \n",
       "CUVHEH_clean                      0.001684  0.000842  0.000421  0.682870  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "invalid-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_input_df_ = l_input_df.copy()\n",
    "l_data = np.log10(l_input_df_)\n",
    "\n",
    "max_v = 2.5\n",
    "min_v = -4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adequate-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_data = (l_data - min_v) / (max_v - min_v)\n",
    "f_data = f_input_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "corporate-watts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_1\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_2\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_3\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_4\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_5\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_6\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_7\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_8\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_9\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n",
      "set_10\n",
      "Direct learning...\n",
      "train data : 216, val data : 24, test data : 60\n"
     ]
    }
   ],
   "source": [
    "for save_dir in save_dir_list:\n",
    "    l_input_df_ = l_input_df_.sample(frac=1)\n",
    "    \n",
    "    print(f\"{save_dir}\")\n",
    "    \n",
    "    if is_transfer:\n",
    "        print(\"Tranfer learning\")\n",
    "        dl_prefix = f\"../../../checkpoint/Direct_learning/size_{n_data}/\" + save_dir\n",
    "\n",
    "        with open(dl_prefix+\"/train_data.pickle\", \"rb\") as f:\n",
    "            train_data = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        with open(dl_prefix+\"/val_data.pickle\", \"rb\") as g:\n",
    "            val_data = pickle.load(g)\n",
    "        g.close()\n",
    "        \n",
    "        with open(dl_prefix+\"/test_data.pickle\", \"rb\") as g:\n",
    "            test_data = pickle.load(g)\n",
    "        g.close()\n",
    "\n",
    "        print(\"train data : {}, val data : {}, test data : {}\".format(len(train_data), \n",
    "                                                                      len(val_data), \n",
    "                                                                      len(test_data)))\n",
    "\n",
    "    else:\n",
    "        print(\"Direct learning...\")\n",
    "        l_data = l_data[:n_data]\n",
    "        # shuffle input data\n",
    "        data_name = []\n",
    "\n",
    "        for name in l_data.index:\n",
    "            if name in f_data.index:\n",
    "                data_name.append(name)\n",
    "\n",
    "        np.random.shuffle(data_name)\n",
    "\n",
    "        n_train = int(0.72*len(data_name))\n",
    "        n_val = int(0.08*len(data_name))\n",
    "        n_test = len(data_name) - n_train - n_val\n",
    "\n",
    "        train_data = data_name[:n_train]\n",
    "        val_data = data_name[n_train:n_train+n_val]\n",
    "        test_data = data_name[-n_test:]\n",
    "\n",
    "        print(\"train data : {}, val data : {}, test data : {}\".format(len(train_data), \n",
    "                                                                      len(val_data), \n",
    "                                                                      len(test_data)))\n",
    "        \n",
    "    # save data for training\n",
    "    if not os.path.exists(prefix+save_dir):\n",
    "        os.makedirs(prefix+save_dir)\n",
    "\n",
    "    with open(\"{}/train_data.pickle\".format(prefix+save_dir), \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    f.close()\n",
    "\n",
    "    with open(\"{}/val_data.pickle\".format(prefix+save_dir), \"wb\") as g:\n",
    "        pickle.dump(val_data, g)\n",
    "    g.close()\n",
    "    \n",
    "    with open(\"{}/test_data.pickle\".format(prefix+save_dir), \"wb\") as g:\n",
    "        pickle.dump(test_data, g)\n",
    "    g.close()\n",
    "\n",
    "    with open(\"{}/training_time.txt\".format(prefix+save_dir), \"w\") as h:\n",
    "        h.write(\"Step\\tTime duration\\tloss(mse)\\n\")\n",
    "    h.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-invasion",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "disciplinary-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_steps = 200000\n",
    "lr = 0.00001\n",
    "optimizer = tf.optimizers.Adam(lr=lr)\n",
    "init_step = 0\n",
    "\n",
    "source_weight_path = f\"../../../checkpoint/source_geo+gridhist/CH4_pm+tb_ai_{source_bar}bar/diffnet-CH4_pm+tb_ai_{source_bar}bar-min.h5\"\n",
    "\n",
    "# min rmse\n",
    "min_loss = 1e30\n",
    "min_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cooked-throw",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_1 starts...\n",
      "load data\n",
      "load model\n",
      "Model: \"diff_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  28672     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  129       \n",
      "=================================================================\n",
      "Total params: 94,465\n",
      "Trainable params: 94,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train starts\n",
      "[1000] Loss Val:  0.0064, MIN MSE (0): 1000000000000000019884624838656.0000\n",
      "NEW MIN LOSS :  0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-00569887fb37>\", line 40, in <module>\n",
      "    train_step_idx_, train_loss_list_, val_step_idx_, val_loss_list_ = training(\n",
      "  File \"<ipython-input-7-f90fe408495f>\", line 21, in training\n",
      "    x, y = next(train_iter)\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 747, in __next__\n",
      "    return self._next_internal()\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 730, in _next_internal\n",
      "    ret = gen_dataset_ops.iterator_get_next(\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2574, in iterator_get_next\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/lim/anaconda3/envs/diffnet/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-00569887fb37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# training stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     train_step_idx_, train_loss_list_, val_step_idx_, val_loss_list_ = training(\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiffnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_transfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_weight_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_weight_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f90fe408495f>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, is_transfer, source_weight_path, n_steps, step, min_step, min_loss, train_iter, val_iter, prefix, save_dir, es, random_state)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    731\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2573\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2574\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2575\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/diffnet/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for save_dir in save_dir_list:\n",
    "    seed_idx = int(save_dir.split(\"_\")[-1])\n",
    "    print(f\"{save_dir} starts...\")\n",
    "    print(\"load data\")\n",
    "    # load train data / val data name\n",
    "    with open(\"{}/train_data.pickle\".format(prefix+save_dir), \"rb\") as f:\n",
    "        train_data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    with open(\"{}/val_data.pickle\".format(prefix+save_dir), \"rb\") as g:\n",
    "        val_data = pickle.load(g)\n",
    "    g.close()\n",
    "    \n",
    "    # load dataloader\n",
    "    dataloader = DataLoader()\n",
    "    \n",
    "    train_f_data, train_l_data = dataloader.arrange_data(f_data, l_data, train_data)\n",
    "    val_f_data, val_l_data = dataloader.arrange_data(f_data, l_data, val_data)\n",
    "    \n",
    "    train_dataset = dataloader.make_dataset(np.array(train_f_data), np.array(train_l_data), \n",
    "                                        batch_size=len(train_l_data), repeat=True, shuffle=True, buffer_size=len(train_l_data))\n",
    "\n",
    "    val_dataset = dataloader.make_dataset(np.array(val_f_data), np.array(val_l_data), \n",
    "                                           batch_size=len(val_l_data), repeat=True, shuffle=False)\n",
    "\n",
    "\n",
    "    train_dataset_iter = iter(train_dataset)\n",
    "    val_dataset_iter = iter(val_dataset)\n",
    "    \n",
    "    # load model\n",
    "    print(\"load model\")\n",
    "    diffnet = DiffNET(input_size=(len(f_data.columns),), dropout=0.5, transfer=is_transfer)\n",
    "    diffnet.initialize_weights()\n",
    "    diffnet.summary()\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "    \n",
    "    print(\"train starts\")\n",
    "    # training stage\n",
    "    train_step_idx_, train_loss_list_, val_step_idx_, val_loss_list_ = training(\n",
    "        model=diffnet, is_transfer=is_transfer, source_weight_path=source_weight_path, \n",
    "        n_steps=n_steps, step=init_step, min_step=min_step, min_loss=min_loss, \n",
    "        train_iter=train_dataset_iter, val_iter=val_dataset_iter, \n",
    "        prefix=prefix, save_dir=save_dir, es=early_stopping, random_state=seed_idx)\n",
    "    \n",
    "    # draw loss plot\n",
    "    plt.figure(figsize=(13,10))\n",
    "    plt.plot(np.array(train_step_idx_) / 1e3, train_loss_list_, label=\"train_loss\")\n",
    "    plt.plot(np.array(val_step_idx_) / 1e3, val_loss_list_, label=\"val_loss\")\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.ylim(0, 0.05)\n",
    "    plt.xlabel(\"Step (x$10^{3}$)\", fontsize=25)\n",
    "    plt.ylabel(\"Loss (MSE)\", fontsize=25)\n",
    "    plt.savefig(\"{}/loss_plot.png\".format(prefix+save_dir), bbox_inches=\"tight\")\n",
    "    #plt.show()\n",
    "    \n",
    "    # load test data\n",
    "    with open(\"{}/test_data.pickle\".format(prefix+save_dir), \"rb\") as g:\n",
    "        test_data = pickle.load(g)\n",
    "    g.close()\n",
    "    \n",
    "    test_f_data, test_l_data = dataloader.arrange_data(f_data, l_data, test_data)\n",
    "    test_dataset = dataloader.make_dataset(np.array(test_f_data), np.array(test_l_data),\n",
    "                           batch_size=len(test_l_data), shuffle=False, repeat=False)\n",
    "\n",
    "    test_dataset = test_dataset.batch(len(test_l_data))\n",
    "    \n",
    "    # load min model\n",
    "    diffnet = DiffNET(input_size=(len(f_data.columns),), transfer=False)\n",
    "    diffnet.initialize_weights()\n",
    "    diffnet.load_weights(\"{}/diffnet-{}-min.h5\".format(prefix+save_dir, save_dir))\n",
    "    \n",
    "    # check mse and r2 score for test data\n",
    "    test_y_pred = []\n",
    "    test_y_true = []\n",
    "\n",
    "    for x, y in test_dataset:\n",
    "        _y = diffnet(x, training=False)\n",
    "        test_y_pred += _y.numpy().reshape(-1).tolist()\n",
    "        test_y_true += y.numpy().reshape(-1).tolist()\n",
    "\n",
    "    test_y_pred = np.array(test_y_pred)\n",
    "    test_y_true = np.array(test_y_true)\n",
    "    \n",
    "    y_true_ = test_y_true * (max_v - min_v) + min_v\n",
    "    y_pred_ = test_y_pred * (max_v - min_v) + min_v\n",
    "\n",
    "    test_mse = cal_mse(y_true=y_true_, y_pred=y_pred_).numpy().item()\n",
    "    test_r2score = r2_score(y_true_, y_pred_)\n",
    "    \n",
    "    plt.figure(figsize=(12,10), dpi=300)\n",
    "    hb = plt.hexbin(y_true_, y_pred_, gridsize=80, cmap=\"bwr\", mincnt=1)\n",
    "    plt.plot([min_v, max_v], [min_v, max_v], color=\"black\", ls=\":\")\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xlim([min_v, max_v])\n",
    "    plt.ylim([min_v, max_v])\n",
    "    plt.xlabel(\"Calculated\", fontsize=25)\n",
    "    plt.ylabel(\"Prediction\", fontsize=25)\n",
    "    #plt.xscale(\"log\")\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.text(min_v+0.1, max_v - 0.5, \"RMSE : {:.4f}\".format(np.sqrt(test_mse)), fontsize=25)\n",
    "    plt.text(min_v+0.1, max_v - 0.8, \"R2 score : {:.4f}\".format(test_r2score), fontsize=25)\n",
    "    cb = plt.colorbar(hb)\n",
    "    cb.set_label(\"Counts\", fontsize=25, rotation=270, labelpad=25)\n",
    "    for t in cb.ax.get_yticklabels():\n",
    "        t.set_fontsize(20)\n",
    "    plt.savefig(\"{}/{}-testset_hexbin.png\".format(prefix+save_dir, save_dir), bbox_inches=\"tight\")\n",
    "    plt.close(\"all\")\n",
    "    #plt.show()\n",
    "    \n",
    "    h = open(\"{}/{}-results.txt\".format(prefix+save_dir, save_dir), \"w\")\n",
    "    h.write(\"RMSE\\tR2_score\\n\")\n",
    "    h.write(f\"{np.sqrt(test_mse):.4f}\\t{test_r2score:.4f}\")\n",
    "    h.close()\n",
    "    \n",
    "    print(f\"{save_dir} ends...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-grove",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
